# -*- coding: utf-8 -*-
"""initial Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GI0C3TdGI_nnJUI-pjBCu0Zo9L99v1uK

##Notebook for Data Collection and Extraction
"""

#Installing popular libraries for use

pip install pandas numpy scikit-learn xgboost

#collecting list of departments in FUTA from https://schools.futa.edu.ng/

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem.porter import *
import string
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix
import seaborn
#from textstat.textstat import *
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import f1_score
from sklearn.feature_selection import SelectFromModel
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
import numpy as np
from nltk.sentiment.vader import SentimentIntensityAnalyzer as VS
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
# %matplotlib inline

csv_file = "course_interests_refined.csv"
df = pd.read_csv(csv_file)

print(f"Loaded {len(df)} rows")
df.head()

df

# Lowercase and strip extra spaces
df['course'] = df['course'].str.strip()
df['interests'] = df['interests'].str.lower().str.strip()

#remove accidental double spaces and punctuations inside 'interests'

df['interests'] = df['interests'].str.replace(r'\s+', ' ', regex=True)

# Save as model-ready CSV
df.to_csv("course_interests_clean.csv", index=False)

print("Preprocessed file saved as 'course_interests_clean.csv'")

# Features (interest keywords) and labels (course names)
X = df["interests"]
y = df["course"]

# Split into train & test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Convert text to TF-IDF features
vectorizer = TfidfVectorizer()
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Define models
models = {
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000)
}

results = {}

# Train & evaluate each model
for name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"\n{name} Accuracy: {acc:.2%}")
    print(classification_report(y_test, y_pred))

"""###This is the main reason why this dataset is not fit for model building (I only did to experiment)

##High Cardinality of Target Labels


We have about 50 rows one course per row


In classification, the model learns to map patterns in interests → course.



If a course appears only once in training, there’s no way the model will recognize it again in testing — it's never seen it before.



This is called the “single instance per class” problem
"""
